{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#python\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.title.string if soup.title else 'No title'\n",
    "            meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "            description = meta_desc['content'] if meta_desc else 'No description'\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = ' '.join([p.get_text() for p in paragraphs])\n",
    "            return title, description, content\n",
    "        else:\n",
    "            return 'No title', 'No description', 'No content'\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch content from {url}: {e}\")\n",
    "        return 'No title', 'No description', 'No content'\n",
    "\n",
    "url = 'https://gganimate.com/'\n",
    "title, description, content = fetch_content(url)\n",
    "print(f'Title: {title}\\nDescription: {description}\\nContent: {content[:1000]}')  # Print first 1000 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch content from https://careers.warnermediagroup.com/TGnewUI/Search/Home/Home?partnerid=391&siteid=36#home: HTTPSConnectionPool(host='careers.warnermediagroup.com', port=443): Max retries exceeded with url: /TGnewUI/Search/Home/Home?partnerid=391&siteid=36 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000237532F6B40>, 'Connection to careers.warnermediagroup.com timed out. (connect timeout=None)'))\n",
      "Failed to fetch content from https://m.animeblix.com/: HTTPSConnectionPool(host='m.animeblix.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000023756172570>: Failed to resolve 'm.animeblix.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Failed to fetch content from https://coworking.munijesusmaria.gob.pe/index.php/login: HTTPSConnectionPool(host='coworking.munijesusmaria.gob.pe', port=443): Max retries exceeded with url: /index.php/login (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000023756274890>, 'Connection to coworking.munijesusmaria.gob.pe timed out. (connect timeout=None)'))\n",
      "Failed to fetch content from http://localhost:7860/: HTTPConnectionPool(host='localhost', port=7860): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023757F4B6E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch content from https://r.prevos.net/tap-water-sentiment-analysis/: HTTPSConnectionPool(host='r.prevos.net', port=443): Max retries exceeded with url: /tap-water-sentiment-analysis/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000023762D7AC60>: Failed to resolve 'r.prevos.net' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions have been added and saved to C:\\Users\\ronal\\OneDrive\\Documentos\\APPs\\Favorites_analysis\\Favorites_data\\bookmarks_with_descriptions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch content from a URL\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.title.string if soup.title else 'No title'\n",
    "            meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "            description = meta_desc['content'] if meta_desc else 'No description'\n",
    "            return title, description\n",
    "        else:\n",
    "            return 'No title', 'No description'\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch content from {url}: {e}\")\n",
    "        return 'No title', 'No description'\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_path = \"C:\\\\Users\\\\ronal\\\\OneDrive\\\\Documentos\\\\APPs\\\\Favorites_analysis\\\\data\\\\bookmarks.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "bookmarks_df = pd.read_csv(csv_path)\n",
    "\n",
    "# List to hold the results\n",
    "results = []\n",
    "\n",
    "# Loop through each URL in the DataFrame\n",
    "for index, row in bookmarks_df.iterrows():\n",
    "    url = row['URL']\n",
    "    title, description = fetch_content(url)\n",
    "    results.append({'URL': url, 'Title': title, 'Description': description})\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Path to the new CSV file\n",
    "new_csv_path = \"C:\\\\Users\\\\ronal\\\\OneDrive\\\\Documentos\\\\APPs\\\\Favorites_analysis\\\\data\\\\bookmarks_with_descriptions.csv\"\n",
    "\n",
    "# Write the new DataFrame to a CSV file\n",
    "results_df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(f\"Descriptions have been added and saved to {new_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract the root domain from a URL\n",
    "def get_root_domain(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        return parsed_url.netloc\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse URL {url}: {e}\")\n",
    "        return 'No domain'\n",
    "\n",
    "# Path to the existing CSV file\n",
    "csv_path = \"C:\\\\Users\\\\ronal\\\\OneDrive\\\\Documentos\\\\APPs\\\\Favorites_analysis\\\\Favorites_data\\\\bookmarks_with_descriptions.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "bookmarks_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Add a new column for the root domain\n",
    "bookmarks_df['Root Domain'] = bookmarks_df['URL'].apply(get_root_domain)\n",
    "\n",
    "# Path to the new CSV file with root domains\n",
    "new_csv_path_with_domains = \"C:\\\\Users\\\\ronal\\\\OneDrive\\\\Documentos\\\\APPs\\\\Favorites_analysis\\\\Favorites_data\\\\bookmarks_with_root_domains.csv\"\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "bookmarks_df.to_csv(new_csv_path_with_domains, index=False)\n",
    "\n",
    "print(f\"Root domains have been added and saved to {new_csv_path_with_domains}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
